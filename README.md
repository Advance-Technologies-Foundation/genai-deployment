# GenAI Setup Guide

---

## Table of Contents

1. [Services Overview](#services-overview)
2. [Installation](#installation)
   - [Install with Docker Compose](#install-with-docker-compose)
     - [Install Docker](#install-docker)  
     - [Install Docker Compose](#install-docker-compose)  
     - [Download Docker Compose Files](#download-docker-compose-files)  
     - [Set Up Container Variables](#set-up-container-variables)  
     - [Run GenAI Service Containers](#run-genai-service-containers)  
     - [Configure GenAI Functionality in Creatio](#configure-genai-functionality-in-creatio)  
     - [Notes](#notes)
   - [Install via Kubernetes](#install-via-kubernetes)
     - [Prerequisites](#prerequisites)
     - [Download Helm Files](#download-helm-files)
     - [Configuration: `values.onsite.yaml`](#configuration-valuesonsiteyaml)
       - [OpenAI Model Configuration](#openai-model-configuration)
       - [Azure Model Configuration](#azure-model-configuration)
       - [Default Model Configuration](#default-model-configuration)
       - [Docker Registry Credentials](#docker-registry-credentials)
       - [Ingress Configuration](#ingress-configuration)
     - [Deploy with Helm](#deploy-with-helm)
     - [Configure GenAI in Creatio](#configure-genai-in-creatio)

---

## Services Overview

This deployment consists of several core services managed via Docker Compose. Each service plays a key role in the functioning of the GenAI system.

### `enrichment-service`
A web service responsible for handling GenAI requests such as **chat completions**.  
It acts as the main REST API endpoint, processing input text from clients (e.g., Creatio) and returning responses generated by an underlying language model.

### `litellm`
A service that provides an abstraction layer for **Large Language Model (LLM)** APIs.

**Purpose:**
- Enables easy integration with multiple LLM providers such as **OpenAI** and **Azure OpenAI**.
- Offers a **unified API interface** for making chat, completion, and embedding requests.
- Simplifies model management, routing, and provider switching with minimal configuration changes.

### `postgres`
A PostgreSQL database used for storing:
- GenAI configuration data (e.g., prompts)
- Operational statistics (e.g., request logs)

## Installation

### Install with Docker Compose

#### Install Docker

Install Docker on a physical or virtual machine running Linux OS to deploy the GenAI components.

Refer to the official Docker installation documentation for Linux (Debian-based distributions):  
https://docs.docker.com/install/linux/docker-ce/debian/

Verify Docker installation by running the following command:

```bash
docker --version
```

---

#### Install Docker Compose

1. Install Docker Compose on your Linux machine.

Refer to the official Docker Compose installation guide:  
https://docs.docker.com/compose/install/

Verify Docker Compose installation by running:

```bash
docker-compose --version
```

---

#### Download Docker Compose files

1. Download and unpack the archive with the setup files from the repository:  
   [GenAI Docker Compose Setup](https://github.com/Advance-Technologies-Foundation/genai-deployment/archive/refs/heads/main.zip)

2. Navigate to the docker-compose folder

---

#### Set Up Container Variables

Configure the GenAI service containers by setting environment variables in the `.env` file located in the deployment folder. Edit this file to provide the required parameters.

##### API Authentication Parameters

Provide parameters depending on the Large Language Model (LLM) service you are using:

##### For OpenAI LLM

| Variable                     | Description                                               |
|------------------------------|-----------------------------------------------------------|
| `OPENAI_MODEL`              | OpenAI provider and model ID (e.g., openai/gpt-4o)         |
| `OPENAI_EMBEDDING_MODEL`    | OpenAI provider and embedding model ID (e.g., openai/gpt-4o)         |
| `OPENAI_API_KEY`              | Your OpenAI API key to authenticate API requests.         |
| `OPENAI_API_KEY_TEXT_EMBEDDING` | *(Optional)* Separate key for OpenAI text embedding service if different from main key. |

##### For Azure OpenAI LLM

| Variable                     | Description                                               |
|------------------------------|-----------------------------------------------------------|
| `AZURE_MODEL`               | Azure provider and model ID (e.g., azure/gpt-4o-2024-11-20)     |
| `AZURE_EMBEDDING_MODEL`     | Azure provider and embedding model ID (e.g., azure/gpt-4o-2024-11-20)      |
| `AZURE_API_KEY`               | Azure API key (subscription key) for authentication.      |
| `AZURE_API_TEXT_EMBEDDING`   | *(Optional)* Separate API key or token for Azure text embedding service. |
| `AZURE_DEPLOYMENTID`          | Deployment ID or name of the Azure OpenAI model to use.   |
| `AZURE_RESOURCENAME`          | Name of your Azure OpenAI resource instance for endpoint construction. |
| `AZURE_API_BASE`          | The base URL of your Azure OpenAI endpoint. This is typically in the format https://<your-resource-name>.openai.azure.com. It's used to construct full API request URLs. |
| `AZURE_EMBEDDING_API_BASE`          | The base URL of your Azure OpenAI endpoint (text embeddings model). This is typically in the format https://<your-resource-name>.openai.azure.com. It's used to construct full API request URLs. |
| `AZURE_API_VERSION`          | The version of the Azure OpenAI API to use. For example: 2023-07-01-preview. This must match a supported version by Azure and may change over time as the API evolves. |

##### Default Models

Set the default models used by GenAI (put model names of the provider that has to be used by default. You can take them from corresponding "openai" or "azure" sections):

| Variable                     | Description                                               |
|------------------------------|-----------------------------------------------------------|
| `GenAI__DefaultModel`         | Identifier or name of the default language generation model (for text completion or chat). Take it from \etc\litellm-config.yaml (model_name parameter of the corresponding model) |
| `GenAI__EmbeddingsModel`      | Identifier or name of the default text embeddings model. Take it from \etc\litellm-config.yaml (model_name parameter of the corresponding model) |

---

#### Log in to Docker Registry
Run this command, replacing your-username and your-password with your actual credentials:

```bash
docker login registry.creatio.com -u your-username -p your-password
```
---

#### Run GenAI Service Containers

1. Open a terminal and navigate to the docker-compose folder.

2. Run the following command to start the GenAI service containers in detached mode:

```bash
docker-compose up -d
```

---

#### Configure GenAI Functionality in Creatio

1. Log into Creatio and go to **System Settings**.
2. Locate the setting: `Account enrichment service url` (code: `AccountEnrichmentServiceUrl`).
3. Set its value using the following format:

```
http://[your_server_ip_address]:5006
```

Replace `[your_server_ip_address]` with the actual IP address or hostname of the server where the GenAI Docker containers are running.

#### Notes

- This deployment uses **Docker Compose** for orchestration.  
- Ensure that the firewall allows inbound traffic on port **5006** for Creatio to communicate with the GenAI service.  
- Keep the `.env` file secure, as it contains sensitive API keys.  

---

### Install via Kubernetes

#### Deployment Overview

The Kubernetes deployment process consists of the following steps:

1. **Prerequisites** - Verify system requirements and tools (~5 minutes)
2. **Download Files** - Get the Helm chart files (~2 minutes)
3. **Configuration** - Configure your `values.onsite.yaml` file (~10-15 minutes)
4. **Deploy** - Install the application with Helm (~3-5 minutes)
5. **Verify Deployment** - Check deployment status and test API endpoint (~2 minutes)
6. **Configure Creatio** - Set up and test the connection in Creatio (~3 minutes)

---

#### Prerequisites

Ensure the following components are set up:

##### System Requirements

- **Kubernetes cluster** (minimum version 1.19+)
- **Helm** package manager (minimum version 3.0+)
- **kubectl** configured to access your cluster
- Valid **credentials** for accessing `registry.creatio.com`

##### Verification Commands

Run these commands to verify your setup:

**Check Kubernetes cluster access:**
```bash
kubectl cluster-info
kubectl get nodes
```

**Check Helm version:**
```bash
helm version
```

**Check Docker registry access:**
```bash
docker login registry.creatio.com -u your-username -p your-password
```

**Expected output:** All commands should complete successfully without errors.

##### Additional Resources

- [Kubernetes Setup Guide](https://kubernetes.io/docs/home/)
- [Helm Installation Guide](https://helm.sh/docs/intro/install/)
- [kubectl Installation Guide](https://kubernetes.io/docs/tasks/tools/install-kubectl/)

---

#### Download Helm Files

1. Download and extract the setup files from the repository:

   [GenAI Kubernetes Setup - GitHub](https://github.com/Advance-Technologies-Foundation/genai-deployment/archive/refs/heads/main.zip)

2. Navigate to the `helm/genai` directory within the extracted archive.

---

#### Step-by-Step Configuration Guide

Follow these steps to configure your `values.onsite.yaml` file for deployment:

##### Configuration Overview

The configuration process involves setting up the following components in order:

1. **Docker Registry Credentials** (Required) - Access to pull container images
2. **LLM Provider Configuration** (Required) - OpenAI, Azure, or both
3. **Service Access Configuration** (Optional) - NodePort or Ingress setup
4. **PostgreSQL Storage** (Optional) - Enable persistent storage if needed

**Important:** You can configure parameters for either OpenAI, Azure, or both. If you are not using one of these providers, the corresponding section should be either commented out or removed entirely.

##### Step 1: Configure Docker Registry Access

First, configure access to the Docker registry to pull container images:

```yaml
dockerRegistry:
  username: <your-username>   
  password: <your-password>   
  email: <your-email>         
```

Replace with your actual credentials for accessing `registry.creatio.com`.

##### Step 2: Configure LLM Provider

Choose one of the following options based on your LLM provider. **Note:** These are example configurations - you need to replace the placeholder values with your actual parameters.

###### Option A: OpenAI Configuration

**Example configuration** (replace with your actual values):

```yaml
appConfig:
  genAI:
    llmProviders:
      models:
        openai:
          - name: gpt-4o
            model: gpt-4o
            api_key: sk-your-openai-api-key  # Replace with your actual OpenAI API key
          # - name: text-embedding-3-small  # Optional embedding model (disabled by default)
          #   model: text-embedding-3-small
          #   api_key: sk-your-openai-api-key  # Replace with your actual OpenAI API key
      defaultModel: gpt-4o
      # embeddingsModel: text-embedding-3-small # Optional - uncomment if using embeddings
```

###### Option B: Azure Configuration

**Example configuration** (replace with your actual values):

```yaml
appConfig:
  genAI:
    llmProviders:
      models:
        azure:
          - name: azure-gpt-4o
            model: gpt-4o-2024-11-20  # Replace with your actual Azure model deployment
            resource_name: your-azure-resource  # Replace with your actual Azure resource name
            api_key: your-azure-api-key  # Replace with your actual Azure API key
          # - name: azure-text-embedding  # Optional embedding model (disabled by default)
          #   model: text-embedding-3-small  # Replace with your actual Azure embedding model
          #   resource_name: your-azure-resource  # Replace with your actual Azure resource name
          #   api_key: your-azure-api-key  # Replace with your actual Azure API key
      defaultModel: azure-gpt-4o
      # embeddingsModel: azure-text-embedding # Optional - uncomment if using embeddings
```

**Parameter descriptions:**

| Parameter | OpenAI | Azure | Description |
|-----------|---------|-------|-------------|
| `name` | Required | Required | Reference name for the model (must match the name used in defaultModel) |
| `model` | Required | Required | Model name (e.g., `gpt-4o`, `gpt-4o-2024-11-20`) |
| `api_key` | Required | Required | API key for authentication |
| `resource_name` | N/A | Required | Azure resource name |

**Important:** The `defaultModel` and `embeddingsModel` values must exactly match the `name` field from the model definitions above.

##### Step 3: Configure Service Access (Optional)

By default, the service is accessible via NodePort on port 30082. Choose one of the following options:

###### Option A: Use NodePort (Default)

No additional configuration needed. The service will be accessible on port 30082.

To change the port:
```yaml
service:
  nodePort: 30082  # Change to your preferred port
```

###### Option B: Enable Ingress

To enable Ingress instead of NodePort:

```yaml
ingress:
  enabled: true
  hosts:
    - genai.example.com  # Replace with your DNS name
```

##### Step 4: Configure PostgreSQL Storage (Optional)

By default, PostgreSQL runs without persistent storage enabled. This is normal behavior since the database only stores non-critical information such as request counts and token usage statistics. 

If persistent storage is needed, add this to your configuration:

```yaml
postgresql:
  primary:
    persistence:
      enabled: true
```

---

#### Deploy with Helm

From the directory containing the `values.onsite.yaml` file, run:

```bash
helm upgrade --install genai . -f values.onsite.yaml
```

---

#### Verify Deployment Status

Before configuring Creatio, verify that your Kubernetes deployment is running successfully:

```bash
# Check if all pods are running
kubectl get pods

# Check service status
kubectl get services
```

**Expected output:** All pods should be in `Running` status and the service should show the NodePort (default: 30082).

##### Determine Service URL

Choose the appropriate URL format based on your configuration:

###### For NodePort (Default):
```
http://<kubernetes_node_ip>:30082
```

To find your Kubernetes node IP:
```bash
kubectl get nodes -o wide
```

###### For Ingress:
```
http://genai.example.com
```
(Use the hostname you configured in your ingress settings)

##### Test API Endpoint

**For NodePort:**
```bash
curl -X GET http://192.168.1.100:30082/health  # Replace 192.168.1.100 with your actual node IP
```

**For Ingress:**
```bash
curl -X GET http://genai.yourdomain.com/health
```

**Expected response:** HTTP 200 with health status information.

---

#### Configure GenAI in Creatio

##### Step 1: Configure System Setting

1. **Log into Creatio** as an administrator
2. Navigate to **System Settings** (System Designer → System Settings)
3. **Search** for code: `GenAIServiceUrl`
4. **Set the value** using the URL determined in the previous section
5. **Save** the configuration

**Example configurations:**
- NodePort: `http://192.168.1.100:30082`  # Replace 192.168.1.100 with your actual node IP
- Ingress: `http://genai.yourdomain.com`  # Replace with your actual domain

##### Step 2: Test Connection in Creatio

1. **Open Creatio AI Chat** feature in your Creatio interface
2. **Send a test message**: "Hi, how are you?"
3. **Verify response**: You should receive a response from the AI assistant

**Expected behavior:** The AI should respond appropriately, indicating the GenAI service is working correctly.

---

If you have any questions or require assistance, please refer to the project repository or contact support.
